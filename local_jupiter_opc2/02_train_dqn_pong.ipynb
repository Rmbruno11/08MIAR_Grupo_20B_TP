{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ce2d02",
   "metadata": {},
   "source": [
    "# 02_train_dqn_pong.ipynb\n",
    "Este notebook define la arquitectura DQN, configura el agente, entrena durante 10M pasos y evalúa en 50 episodios para obtener la media de recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca193e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports esenciales\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "from rl.core import Processor\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Permute, Conv2D, BatchNormalization, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers y Processor\n",
    "def wrap_env(env):\n",
    "    env = AtariPreprocessing(env,\n",
    "                             frame_skip=4,\n",
    "                             screen_size=84,\n",
    "                             grayscale_obs=True,\n",
    "                             scale_obs=True)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    return env\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return observation\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch.astype('float32') / 255.0\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2132f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros globales y entorno\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "env = wrap_env(gym.make('PongDeterministic-v4'))\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura CNN para DQN\n",
    "model = Sequential([\n",
    "    Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE),\n",
    "    Conv2D(32, (8, 8), strides=4, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(nb_actions, activation='linear')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11600979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del agente DQN\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=1.0,\n",
    "    value_min=0.001,\n",
    "    value_test=0.001,\n",
    "    nb_steps=2_000_000\n",
    ")\n",
    "memory = SequentialMemory(limit=2_000_000, window_length=WINDOW_LENGTH)\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    processor=AtariProcessor(),\n",
    "    nb_steps_warmup=100_000,\n",
    "    gamma=0.995,\n",
    "    train_interval=2,\n",
    "    target_model_update=5_000,\n",
    "    delta_clip=1.0,\n",
    "    enable_double_dqn=True,\n",
    "    enable_dueling_network=True,\n",
    "    dueling_type='max',\n",
    "    dueling_size=512\n",
    ")\n",
    "dqn.compile(Adam(learning_rate=6.25e-5), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31290b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento extendido (10M pasos)\n",
    "dqn.fit(\n",
    "    env,\n",
    "    nb_steps=10_000_000,\n",
    "    log_interval=250_000,\n",
    "    verbose=2\n",
    ")\n",
    "dqn.save_weights('dqn_pong_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación en modo test (50 episodios)\n",
    "results = dqn.test(\n",
    "    env,\n",
    "    nb_episodes=50,\n",
    "    visualize=False,\n",
    "    policy=EpsGreedyQPolicy(eps=0.001)\n",
    ")\n",
    "import numpy as np\n",
    "rewards = results.history['episode_reward']\n",
    "print(\"Recompensa media:\", np.mean(rewards))\n",
    "print(\"Desviación estándar:\", np.std(rewards))\n",
    "print(\"Recompensa máxima:\", np.max(rewards))\n",
    "print(\"Recompensa mínima:\", np.min(rewards))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
