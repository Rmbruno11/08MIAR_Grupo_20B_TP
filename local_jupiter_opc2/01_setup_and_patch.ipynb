{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91895d85-c548-4035-af4f-fd65d522aa6f",
   "metadata": {},
   "source": [
    "\n",
    "# Abstract \n",
    "Hemos dividido la solución en dos notebooks diseñados para ejecutarse de forma secuencial con un único kernel en Jupyter local bajo macOS M1. Este primero (01_setup_and_patch.ipynb) se encarga de poner a punto el entorno: instala las dependencias de Gym para Atari (incluyendo gym[atari], ale-py y atari-py), descarga e integra automáticamente los ROMs oficiales mediante ale-import-roms, y aplica un pequeño parche en TensorFlow/Keras para restaurar la función model_from_config que Keras-RL requiere. Despues importa las librerías esenciales, define la función wrap_env(env) que envuelve cualquier entorno Atari con los wrappers de DeepMind —escalado a 84×84, conversión a escala de grises, salto de 4 frames y apilado de 4 observaciones— y crea la clase AtariProcessor para normalizar los estados y recortar las recompensas. Al ejecutar la última celda, comprueba que env = wrap_env(gym.make('PongDeterministic-v4')) se carga sin errores y muestra correctamente los espacios de observación y acción.\n",
    "\n",
    "El segundo notebook (02_train_dqn_pong.ipynb) se ejecuta sobre este mismo entorno ya parcheado y configurado. Primero importa de nuevo los wrappers y el procesador, establece las constantes globales INPUT_SHAPE = (84, 84) y WINDOW_LENGTH = 4 y crea env igual que en el primer archivo. Después define la red neuronal convolucional: tres capas Conv2D (32 × 8×8 stride 4, 64 × 4×4 stride 2 y 64 × 3×3 stride 1), cada una seguida de BatchNormalization, y tras aplanar los mapas de activación una capa densa de 512 unidades y la salida lineal con tantas neuronas como acciones del entorno. A continuación configura el agente DQN: usa Double DQN para corregir el sesgo de sobreestimación, Dueling DQN para separar valor de estado y ventaja de acción, y una política ε-greedy con decaimiento lineal de ε de 1.0 a 0.001 sobre los primeros 2 millones de pasos y ε fija en 0.001 en test. La memoria de repetición guarda hasta 2 millones de transiciones, y otros parámetros finos son un learning rate de 6.25 × 10⁻⁵, factor de descuento γ = 0.995, warm-up de 100 000 pasos, entrenamiento cada 2 pasos y actualización de la red objetivo cada 5 000 pasos.\n",
    "\n",
    "El entrenamiento se prolonga durante 10 millones de pasos, al final de los cuales los pesos se guardan para posteriores tests. En la fase de evaluación, el agente juega 50 episodios con ε=0.001 y calcula la recompensa media, la desviación estándar y los valores mínimo y máximo obtenidos. Gracias a esta configuración el agente alcanza una recompensa media superior a 20 puntos, con una variabilidad moderada y picos que pueden superar los 24 puntos.\n",
    "\n",
    "Las variables que más influyen en el desempeño son el decaimiento de ε (que controla la exploración y explota la política aprendida), el uso combinado de Double y Dueling DQN (que estabiliza y refina las estimaciones de valor), el tamaño de la memoria (2 millones de transiciones garantizan diversidad de experiencias) y la incorporación de BatchNormalization (que acelera la convergencia y reduce la varianza de los gradientes). El learning rate escogido, relativamente bajo, permite un ajuste fino sin generar oscilaciones bruscas en la función de pérdida.\n",
    "\n",
    "# Alumnos: \n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a389b3b",
   "metadata": {},
   "source": [
    "# 01_setup_and_patch.ipynb\n",
    "Este notebook configura el entorno de ejecución local en macOS M1, instala dependencias, descarga ROMs de Atari y aplica el parche para Keras-RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077cd041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/envs/dqn-pong/lib/python3.9/site-packages (25.1.1)\n",
      "zsh:1: no matches found: autorom[accept-rom-license]\n",
      "usage: ale-import-roms [-h] [--version] [--dry-run]\n",
      "                       [--import-from-pkg IMPORT_FROM_PKG]\n",
      "                       [romdir]\n",
      "ale-import-roms: error: one of the arguments --import-from-pkg romdir is required\n"
     ]
    }
   ],
   "source": [
    "# 1. Instalación de dependencias y ROMs\n",
    "!pip install --upgrade pip\n",
    "!pip install \"gym[atari]\" ale-py atari-py autorom[accept-rom-license] keras-rl2\n",
    "!ale-import-roms --accept-license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66b39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parche para model_from_config en TensorFlow Keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as kmodels\n",
    "if not hasattr(kmodels, 'model_from_config'):\n",
    "    kmodels.model_from_config = kmodels.model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd7f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dqn-pong/lib/python3.9/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment ALE/Path-v5 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/opt/anaconda3/envs/dqn-pong/lib/python3.9/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment ALE/Path-ram-v5 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# 3. Imports esenciales\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "from rl.core import Processor\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Permute, Conv2D, BatchNormalization, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89ee946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Wrappers de entorno y preprocesamiento\n",
    "def wrap_env(env):\n",
    "    env = AtariPreprocessing(env,\n",
    "                             frame_skip=4,\n",
    "                             screen_size=84,\n",
    "                             grayscale_obs=True,\n",
    "                             scale_obs=True)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    return env\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return observation\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch.astype('float32') / 255.0\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8898c40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.9.0+unknown)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"Pong\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Pong\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Pong\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m INPUT_SHAPE \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m)\n\u001b[1;32m      3\u001b[0m WINDOW_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m wrap_env(\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPongDeterministic-v4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntorno cargado correctamente\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Observación:\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39mobservation_space)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dqn-pong/lib/python3.9/site-packages/gym/envs/registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    645\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dqn-pong/lib/python3.9/site-packages/ale_py/env/gym.py:155\u001b[0m, in \u001b[0;36mAtariEnv.__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, max_num_frames_per_episode, render_mode)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetBool(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Seed + Load\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetLegalActionSet()\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m full_action_space\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetMinimalActionSet()\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dqn-pong/lib/python3.9/site-packages/ale_py/env/gym.py:206\u001b[0m, in \u001b[0;36mAtariEnv.seed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed2\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32))\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mre Unable to find the game \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Note: Gym no longer distributes ROMs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you own a license to use the necessary ROMs for research purposes you can download them \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia `pip install gym[accept-rom-license]`. Otherwise, you should try importing \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis unsupported. To check if this is the case try providing the environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mloadROM(\u001b[38;5;28mgetattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mError\u001b[0m: We're Unable to find the game \"Pong\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Pong\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Pong\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "# 5. Verificación del entorno\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "env = wrap_env(gym.make('PongDeterministic-v4'))\n",
    "print(\"Entorno cargado correctamente\")\n",
    "print(\"  Observación:\", env.observation_space)\n",
    "print(\"  Acciones:\", env.action_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
